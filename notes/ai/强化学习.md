# [26.01.01] 强化学习

## 无监督学习
有监督学习->有期望结果
无监督学习->没有期望结果，不知道最佳输出是什么

模型可以根据环境互动，判断结果的好坏

On-Policy，被训练的Actor和环境互动的Actor是一个；
Off-Polocy，和环境互动的Actor是被训练Actor的近似，被训练的Actor可能更新了参数，但是没有同步到和环境互动的Actor；

PPO(Proximal Policy Optimization)，




MFU实际上指示的是硬件有多少时间在满负荷计算，这个在预训练阶段是比较有效的，原因在于，对一个样本来说包含完整的问题和回答，预训练的掩码机制可以并行算出来回答中所有位置的输出，实际情况中是一次很大的矩阵计算，而没有一次次迭代生成Token，这种情况MFU就可以直接和训练效率挂钩。
在强化学习阶段，主要流程是Actor针对问题进行推理，每轮只能生成一个Token，直到生成完整答案，这个推理过程消耗是很大的。在有完整答案之后，Critic和Reference模型就可以回答预训练的模式，一次计算所有位置的概率和优势，Award也可以基于完整的回答进行打分。在这个流程里面，推理需要一轮一轮生成答案，占到了总时间的80%以上，所以推理的效率变得非常关键。

强化学习阶段：
- 从题库读取问题，模型针对问题生成完整回答，在这个过程中Actor和Reference都会执行一遍推理；
- 对于actor生成的每个回答，reward模型会打分，根据打分调整模型参数(反向传播，更新参数)